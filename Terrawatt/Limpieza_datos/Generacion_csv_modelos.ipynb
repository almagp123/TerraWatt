{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos hecho en el notebook de los datos meteorologicos vamos primeramente a obtener los nombres de las provincias para poder ver si alguna errata o alguna duplicada, de todos modos vamos a usar un diccionario tmbién para que tengan exactamente el mismo nombre que en el dataset de los datos meteorologico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provincias únicas encontradas:\n",
      "Albacete\n",
      "Alicante\n",
      "Almería\n",
      "Asturias\n",
      "Badajoz\n",
      "Baleares\n",
      "Barcelona\n",
      "Burgos\n",
      "Cantabria\n",
      "Castellón\n",
      "Ciudad Real\n",
      "Cuenca\n",
      "Cáceres\n",
      "Cádiz\n",
      "Córdoba\n",
      "Gerona\n",
      "Granada\n",
      "Guadalajara\n",
      "Guipúzcoa\n",
      "Huelva\n",
      "Huesca\n",
      "Jaén\n",
      "La Coruña\n",
      "La Rioja\n",
      "Las Palmas\n",
      "León\n",
      "Lugo\n",
      "Lérida\n",
      "Madrid\n",
      "Murcia\n",
      "Málaga\n",
      "Navarra\n",
      "Orense\n",
      "Palencia\n",
      "Pontevedra\n",
      "Salamanca\n",
      "Santa Cruz de Tenerife\n",
      "Segovia\n",
      "Sevilla\n",
      "Soria\n",
      "Tarragona\n",
      "Teruel\n",
      "Toledo\n",
      "Valencia\n",
      "Valladolid\n",
      "Vizcaya\n",
      "Zamora\n",
      "Zaragoza\n",
      "Álava\n",
      "Ávila\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import unidecode\n",
    "file_path = \"Datos_brutos_generales/Consumo_Energetico_Viviendas.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path, delimiter=';', encoding='utf-8', engine='python')\n",
    "\n",
    "    # Verificar si la columna \"Provincia\" existe\n",
    "    if \"Provincia\" in df.columns:\n",
    "        # Obtener las provincias únicas\n",
    "        provincias_unicas = df[\"Provincia\"].dropna().unique()\n",
    "\n",
    "        # Mostrar las provincias únicas ordenadas alfabéticamente\n",
    "        print(\"Provincias únicas encontradas:\")\n",
    "        for provincia in sorted(provincias_unicas):\n",
    "            print(provincia)\n",
    "    else:\n",
    "        print(\"La columna 'Provincia' no existe en el archivo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver hay varias de ellas que no reconocer correctamente, por lo que sustituimos por los mismos nombres que hemos utilizado en el anterior dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta creada: Datos_limpios_generales\n",
      "Archivo actualizado guardado en: Datos_limpios_generales\\Consumo_Energetico_Viviendas_Limpio.csv\n",
      "ALAVA\n",
      "ALBACETE\n",
      "ALICANTE\n",
      "ALMERIA\n",
      "ASTURIAS\n",
      "AVILA\n",
      "BADAJOZ\n",
      "BALEARES\n",
      "BARCELONA\n",
      "BURGOS\n",
      "CACERES\n",
      "CADIZ\n",
      "CANTABRIA\n",
      "CASTELLON\n",
      "CIUDAD REAL\n",
      "CORDOBA\n",
      "CUENCA\n",
      "GERONA\n",
      "GRANADA\n",
      "GUADALAJARA\n",
      "GUIPUZCOA\n",
      "HUELVA\n",
      "HUESCA\n",
      "JAEN\n",
      "LA CORUNA\n",
      "LA RIOJA\n",
      "LAS PALMAS\n",
      "LEON\n",
      "LERIDA\n",
      "LUGO\n",
      "MADRID\n",
      "MALAGA\n",
      "MURCIA\n",
      "NAVARRA\n",
      "ORENSE\n",
      "PALENCIA\n",
      "PONTEVEDRA\n",
      "SALAMANCA\n",
      "SANTA CRUZ DE TENERIFE\n",
      "SEGOVIA\n",
      "SEVILLA\n",
      "SORIA\n",
      "TARRAGONA\n",
      "TERUEL\n",
      "TOLEDO\n",
      "VALENCIA\n",
      "VALLADOLID\n",
      "VIZCAYA\n",
      "ZAMORA\n",
      "ZARAGOZA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "\n",
    "file_path = \"Datos_brutos_generales/Consumo_Energetico_Viviendas.csv\"\n",
    "\n",
    "# Verificar si el archivo existe antes de intentar leerlo\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"El archivo {file_path} no existe.\")\n",
    "else:\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(file_path, delimiter=';', encoding='utf-8', engine='python')\n",
    "\n",
    "    # Verificar y transformar la columna 'Provincia'\n",
    "    if \"Provincia\" in df.columns:\n",
    "        df[\"Provincia\"] = df[\"Provincia\"].apply(lambda x: unidecode.unidecode(x).upper().strip())\n",
    "\n",
    "    # Crear la carpeta si no existe\n",
    "    output_folder = \"Datos_limpios_generales\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Carpeta creada: {output_folder}\")\n",
    "\n",
    "    # Guardar el archivo actualizado\n",
    "    output_path = os.path.join(output_folder, \"Consumo_Energetico_Viviendas_Limpio.csv\")\n",
    "    df.to_csv(output_path, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Archivo actualizado guardado en: {output_path}\")\n",
    "\n",
    "    # Verificar las provincias únicas después de la corrección\n",
    "    provincias_corregidas = df[\"Provincia\"].dropna().unique()\n",
    "\n",
    "    for provincia in sorted(provincias_corregidas):\n",
    "        print(provincia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que para ambos csv que vamos a unir tienen los mismos nombres de las provincias, para no tener ningun tipo de problema cuando las vayamos a unir, además te escribe que provincias no son semejantes en caso de que haya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provincias presentes en Datos_limpios_meteorologicos pero no en Consumo_Energetico_Viviendas_Corregido:\n",
      "A CORUÑA\n",
      "BIZKAIA\n",
      "GIPUZKOA\n",
      "GIRONA\n",
      "ILLES BALEARS\n",
      "LLEIDA\n",
      "OURENSE\n",
      "\n",
      "Provincias presentes en Consumo_Energetico_Viviendas_Corregido pero no en Datos_limpios_meteorologicos:\n",
      "BALEARES\n",
      "GERONA\n",
      "GUIPUZCOA\n",
      "LA CORUNA\n",
      "LERIDA\n",
      "ORENSE\n",
      "VIZCAYA\n"
     ]
    }
   ],
   "source": [
    "consumo_file = \"Datos_limpios_generales/Consumo_Energetico_Viviendas_Limpio.csv\"\n",
    "meteorologicos_folder = \"Datos_limpios_meteorologicos\"\n",
    "\n",
    "try:\n",
    "    consumo_df = pd.read_csv(consumo_file, delimiter=';', encoding='latin1', engine='python')\n",
    "    provincias_consumo = set(consumo_df[\"Provincia\"].dropna().unique())\n",
    "except Exception as e:\n",
    "    print(f\"Error leyendo el archivo {consumo_file}: {e}\")\n",
    "    provincias_consumo = set()\n",
    "\n",
    "provincias_meteorologicos = set()\n",
    "\n",
    "try:\n",
    "    for file_name in os.listdir(meteorologicos_folder):\n",
    "        if file_name.endswith(\".csv\"):  \n",
    "            # Extraer el nombre de la provincia desde el nombre del archivo (sin extensión)\n",
    "            provincia = os.path.splitext(file_name)[0].upper() \n",
    "            provincias_meteorologicos.add(provincia)\n",
    "except Exception as e:\n",
    "    print(f\"Error leyendo los archivos en la carpeta {meteorologicos_folder}: {e}\")\n",
    "\n",
    "# Comparar las provincias entre ambos conjuntos\n",
    "provincias_faltantes_en_consumo = provincias_meteorologicos - provincias_consumo\n",
    "provincias_faltantes_en_meteorologicos = provincias_consumo - provincias_meteorologicos\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"\\nProvincias presentes en Datos_limpios_meteorologicos pero no en Consumo_Energetico_Viviendas_Corregido:\")\n",
    "for provincia in sorted(provincias_faltantes_en_consumo):\n",
    "    print(provincia)\n",
    "\n",
    "print(\"\\nProvincias presentes en Consumo_Energetico_Viviendas_Corregido pero no en Datos_limpios_meteorologicos:\")\n",
    "for provincia in sorted(provincias_faltantes_en_meteorologicos):\n",
    "    print(provincia)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver no coinciden todas así que vamos a hacer un mapeo de aquellas no coincidan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo corregido guardado en: Datos_limpios_generales/Consumo_Energetico_Viviendas_Limpio.csv\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV\n",
    "file_path = \"Datos_limpios_generales/Consumo_Energetico_Viviendas_Limpio.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';', encoding='utf-8', engine='python')\n",
    "\n",
    "# Diccionario de mapeo para corregir los nombres de las provincias\n",
    "mapeo_provincias = {\n",
    "    \"BALEARES\": \"ILLES BALEARS\",\n",
    "    \"GERONA\": \"GIRONA\",\n",
    "    \"GUIPUZCOA\": \"GIPUZKOA\",\n",
    "    \"LA CORUNA\": \"A CORUÑA\",\n",
    "    \"LERIDA\": \"LLEIDA\",\n",
    "    \"ORENSE\": \"OURENSE\",\n",
    "    \"VIZCAYA\": \"BIZKAIA\"\n",
    "}\n",
    "\n",
    "# Aplicar el mapeo a la columna \"Provincia\"\n",
    "if \"Provincia\" in df.columns:\n",
    "    df[\"Provincia\"] = df[\"Provincia\"].replace(mapeo_provincias)\n",
    "\n",
    "# Guardar el archivo actualizado\n",
    "df.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Archivo corregido guardado en: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos el dataset del primer modelo ya que vamos a realizar dos modelos, uno para la prediccion del precio de la electricidad y otro para la prediccion del consumo. En este caso para como ya tenemos copiado en el modelo los datos de consumo ahora vamos a añadir los datos metereologicos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado en: Modelo_Consumo_Met_fest.csv\n"
     ]
    }
   ],
   "source": [
    "modelo_path = \"Datos_limpios_generales/Consumo_Energetico_Viviendas_Limpio.csv\"\n",
    "meteorologicos_folder = \"Datos_limpios_meteorologicos\"\n",
    "output_path = 'Modelo_Consumo_Met_fest.csv'\n",
    "\n",
    "df_modelo = pd.read_csv(modelo_path, delimiter=';', encoding='utf-8')\n",
    "\n",
    "df_resultado = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(meteorologicos_folder):\n",
    "    if file_name.endswith(\".csv\"): \n",
    "        provincia = file_name.replace(\".csv\", \"\").upper()  # Obtener el nombre de la provincia desde el archivo\n",
    "        meteorologico_path = os.path.join(meteorologicos_folder, file_name)\n",
    "        \n",
    "        # Leer el archivo meteorológico\n",
    "        df_meteorologico = pd.read_csv(meteorologico_path, delimiter=';', encoding='utf-8')\n",
    "        \n",
    "        # Convertir la columna FECHA a datetime\n",
    "        if 'FECHA' in df_meteorologico.columns:\n",
    "            df_meteorologico['FECHA'] = pd.to_datetime(df_meteorologico['FECHA'], format='%Y-%m-%d', errors='coerce')\n",
    "        \n",
    "        # Filtrar las filas de modelo para la provincia actual\n",
    "        df_modelo_provincia = df_modelo[df_modelo['Provincia'].str.upper() == provincia].copy()\n",
    "        \n",
    "        # Si hay datos para la provincia en el modelo\n",
    "        if not df_modelo_provincia.empty:\n",
    "            # Convertir la columna Fecha a datetime\n",
    "            if 'Fecha' in df_modelo_provincia.columns:\n",
    "                df_modelo_provincia['Fecha'] = pd.to_datetime(df_modelo_provincia['Fecha'], format='%Y-%m-%d', errors='coerce')\n",
    "            \n",
    "            # Unir los datos meteorológicos con el modelo usando la fecha\n",
    "            df_combinado = pd.merge(df_modelo_provincia, df_meteorologico, how='left', left_on='Fecha', right_on='FECHA')\n",
    "            \n",
    "            # Añadir los datos combinados al resultado\n",
    "            df_resultado = pd.concat([df_resultado, df_combinado], ignore_index=True)\n",
    "\n",
    "# Eliminar la columna duplicada 'FECHA' si existe en el resultado\n",
    "if 'FECHA' in df_resultado.columns:\n",
    "    df_resultado = df_resultado.drop(columns=['FECHA'])\n",
    "\n",
    "# Guardar el archivo combinado\n",
    "df_resultado.to_csv(output_path, sep=';', index=False, encoding='utf-8')\n",
    "print(f\"Archivo combinado guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos las variables de si es festivo y si es un día entre semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo actualizado con columna 'Festivo' y 'Entre semana' guardado en: Modelo_Consumo_Met_fest.csv\n"
     ]
    }
   ],
   "source": [
    "# Rutas de los archivos\n",
    "consumo_path = \"Modelo_Consumo_Met_fest.csv\"\n",
    "festivos_path = \"Datos_brutos_generales/Festivos.csv\"\n",
    "\n",
    "# Leer los datasets\n",
    "df_consumo = pd.read_csv(consumo_path, delimiter=';', encoding='utf-8')\n",
    "df_festivos = pd.read_csv(festivos_path, delimiter=';', encoding='utf-8')\n",
    "\n",
    "# Estandarizar las columnas \"Provincia\" y \"Fecha\"\n",
    "df_consumo[\"Provincia\"] = df_consumo[\"Provincia\"].str.strip().str.upper()\n",
    "df_festivos[\"Provincia\"] = df_festivos[\"Provincia\"].str.strip().str.upper()\n",
    "\n",
    "# Convertir las fechas de ambos datasets al mismo formato\n",
    "df_consumo[\"Fecha\"] = pd.to_datetime(df_consumo[\"Fecha\"], format='%Y-%m-%d', errors='coerce')\n",
    "df_festivos[\"Fecha\"] = pd.to_datetime(df_festivos[\"Fecha\"], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Crear una clave de combinación basada en Fecha y Provincia\n",
    "df_consumo[\"Clave\"] = df_consumo[\"Fecha\"].astype(str) + \"_\" + df_consumo[\"Provincia\"]\n",
    "df_festivos[\"Clave\"] = df_festivos[\"Fecha\"].astype(str) + \"_\" + df_festivos[\"Provincia\"]\n",
    "\n",
    "# Añadir la columna \"Festivo\" al dataset de consumo\n",
    "df_consumo[\"Festivo\"] = df_consumo[\"Clave\"].isin(df_festivos[\"Clave\"]).map({True: \"SI\", False: \"NO\"})\n",
    "\n",
    "# Añadir la columna \"Día de la semana\"\n",
    "# 0 = Lunes, ..., 4 = Viernes (Entre semana), 5 = Sábado, 6 = Domingo (Fin de semana)\n",
    "df_consumo[\"Entre semana\"] = df_consumo[\"Fecha\"].dt.dayofweek.apply(lambda x: \"SI\" if x < 5 else \"NO\")\n",
    "\n",
    "# Eliminar la columna auxiliar \"Clave\"\n",
    "df_consumo.drop(columns=[\"Clave\"], inplace=True)\n",
    "\n",
    "# Guardar el archivo actualizado\n",
    "df_consumo.to_csv(consumo_path, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Archivo actualizado con columna 'Festivo' y 'Entre semana' guardado en: {consumo_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos a realizar el segundo modelo que constará de los precios de la energía, los datos metereologicos y, como en el dataset anterior, añadiremos las columnas Festivo y Entre semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado en: Modelo_Precios_Met_Fest.csv\n"
     ]
    }
   ],
   "source": [
    "# Ruta de los archivos\n",
    "meteorologicos_path = \"Datos_limpios_meteorologicos\"\n",
    "precios_path = \"Datos_brutos_generales/precios_energia.csv\"\n",
    "precios_modificado_path = \"Datos_limpios_generales/precios_energia_separador_modificado.csv\"\n",
    "festivos_path = \"Datos_brutos_generales/Festivos.csv\"\n",
    "\n",
    "# Leer los festivos\n",
    "df_festivos = pd.read_csv(festivos_path, delimiter=';', encoding='utf-8')\n",
    "\n",
    "# Estandarizar las columnas \"Provincia\" y \"Fecha\" en el dataset de festivos\n",
    "df_festivos[\"Provincia\"] = df_festivos[\"Provincia\"].str.strip().str.upper()\n",
    "df_festivos[\"Fecha\"] = pd.to_datetime(df_festivos[\"Fecha\"], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Modificar el delimitador del archivo de precios\n",
    "df_precios = pd.read_csv(precios_path, delimiter=',', encoding='utf-8')  # Leer con coma como delimitador\n",
    "df_precios.to_csv(precios_modificado_path, sep=';', index=False, encoding='utf-8')  # Guardar con punto y coma como delimitador\n",
    "\n",
    "# Volver a cargar el archivo con el nuevo delimitador\n",
    "df_precios = pd.read_csv(precios_modificado_path, delimiter=';', encoding='utf-8')\n",
    "\n",
    "# Convertir la columna \"Fecha\" a formato datetime en precios\n",
    "df_precios[\"Fecha\"] = pd.to_datetime(df_precios[\"Fecha\"], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Inicializar una lista para almacenar los DataFrames combinados\n",
    "combined_dfs = []\n",
    "\n",
    "# Procesar cada archivo en la carpeta de datos meteorológicos\n",
    "for file_name in os.listdir(meteorologicos_path):\n",
    "    if file_name.endswith(\".csv\"):  # Asegurarse de que sean archivos CSV\n",
    "        file_path = os.path.join(meteorologicos_path, file_name)\n",
    "\n",
    "        # Extraer el nombre de la provincia del archivo (sin extensión)\n",
    "        provincia = os.path.splitext(file_name)[0].upper()\n",
    "\n",
    "        # Leer el archivo meteorológico\n",
    "        df_meteorologico = pd.read_csv(file_path, delimiter=';', encoding='utf-8')\n",
    "\n",
    "        # Convertir la columna \"FECHA\" a formato datetime en meteorológicos\n",
    "        df_meteorologico[\"FECHA\"] = pd.to_datetime(df_meteorologico[\"FECHA\"], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "        # Añadir la columna Provincia al DataFrame meteorológico\n",
    "        df_meteorologico.insert(1, \"Provincia\", provincia)  # Añadir la columna como segunda\n",
    "\n",
    "        # Combinar con el archivo de precios de energía mediante la columna \"Fecha\"\n",
    "        df_combinado = pd.merge(df_meteorologico, df_precios, left_on=\"FECHA\", right_on=\"Fecha\", how=\"left\")\n",
    "\n",
    "        # Eliminar la columna redundante \"Fecha\" después de la combinación\n",
    "        df_combinado.drop(columns=[\"Fecha\"], inplace=True)\n",
    "\n",
    "        # Crear una clave de combinación basada en Fecha y Provincia para identificar festivos\n",
    "        df_combinado[\"Clave\"] = df_combinado[\"FECHA\"].astype(str) + \"_\" + df_combinado[\"Provincia\"]\n",
    "\n",
    "        # Añadir la columna \"Festivo\"\n",
    "        df_combinado[\"Festivo\"] = df_combinado[\"Clave\"].isin(\n",
    "            df_festivos[\"Fecha\"].astype(str) + \"_\" + df_festivos[\"Provincia\"]\n",
    "        ).map({True: \"SI\", False: \"NO\"})\n",
    "\n",
    "        # Añadir la columna \"Entre semana\"\n",
    "        df_combinado[\"Entre semana\"] = df_combinado[\"FECHA\"].dt.dayofweek.apply(lambda x: \"SI\" if x < 5 else \"NO\")\n",
    "\n",
    "        # Eliminar la columna auxiliar \"Clave\"\n",
    "        df_combinado.drop(columns=[\"Clave\"], inplace=True)\n",
    "\n",
    "        # Añadir al listado de DataFrames combinados\n",
    "        combined_dfs.append(df_combinado)\n",
    "\n",
    "# Concatenar todos los DataFrames combinados\n",
    "df_final = pd.concat(combined_dfs, ignore_index=True)\n",
    "\n",
    "# Dropear las columnas de precios que no sean necesarias\n",
    "columns_to_drop = [\n",
    "    \"Precio medio diario sin impuestos (€/MWh)\",\n",
    "    \"Impuesto eléctrico (€/MWh)\",\n",
    "    \"IVA (€/MWh)\"\n",
    "]\n",
    "df_final.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Guardar el archivo combinado\n",
    "output_path = \"Modelo_Precios_Met_Fest.csv\"\n",
    "df_final.to_csv(output_path, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Archivo combinado guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos de rellenar todas las filas que no contengan datos, en este caso lo rellenaremos con la media de la columna (ya que el unico dataset que tiene datos faltantes es el de datos metereologicos), paorque elastic search no admite que tenga elementos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alma\\AppData\\Local\\Temp\\ipykernel_28292\\3884282466.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mean_value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado como: Modelo_Consumo_Met_fest.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alma\\AppData\\Local\\Temp\\ipykernel_28292\\3884282466.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mean_value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado como: Modelo_Precios_Met_Fest.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Lista de archivos a procesar\n",
    "files = [\"Modelo_Consumo_Met_fest.csv\", \"Modelo_Precios_Met_Fest.csv\"]\n",
    "\n",
    "# Procesar cada archivo\n",
    "for file in files:\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(file, delimiter=';')\n",
    "    \n",
    "    # Eliminar filas donde todas las columnas tengan valores faltantes\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    \n",
    "    # Reemplazar valores faltantes en columnas numéricas con la media de la columna\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_columns:\n",
    "        mean_value = df[col].mean()  # Calcular la media de la columna\n",
    "        df[col].fillna(mean_value, inplace=True)\n",
    "    \n",
    "    # Guardar el archivo procesado con un prefijo \"Procesado_\"\n",
    "    output_file = f\"{file}\"\n",
    "    df.to_csv(output_file, index=False, sep=';')\n",
    "    print(f\"Archivo procesado y guardado como: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
